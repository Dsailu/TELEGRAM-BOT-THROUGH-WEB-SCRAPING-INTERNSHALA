{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5568/1755568558.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import urllib\n",
    "from urllib.parse import unquote\n",
    "import re\n",
    "import time\n",
    "import urllib.request\n",
    "import requests\n",
    "# import urllib.urlparse\n",
    "# import urllib.Request\n",
    "# import urllib.HTTPError\n",
    "from urllib.error import URLError, HTTPError\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.error import URLError, HTTPError\n",
    "from http.client import BadStatusLine\n",
    "import urllib.request,re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import csv\n",
    "import ssl\n",
    "import json\n",
    "\n",
    "base_url = \"https://internshala.com/internships/work-from-home-jobs\"\n",
    "\n",
    "# For loop to generate the list of pages #\n",
    "url_pages = []\n",
    "for i in range(0,200):\n",
    "    count = 0\n",
    "    url_pages.append(i+count)\n",
    "\n",
    "urls = []\n",
    "for idx in url_pages:\n",
    "    skills = []\n",
    "    r = requests.get(base_url+\"/page\"+\"-\"+str(idx))\n",
    "#     print(base_url+\"/page\"+\"-\"+str(idx))\n",
    "    url = base_url+\"/page\"+\"-\"+str(idx)\n",
    "    source = \"Internshala\"\n",
    "    date = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    urls.append((url,source,date))\n",
    "    print(\"Starting Database Intersection\")\n",
    "    try:\n",
    "        connection = psycopg2.connect(user=\"shankar\",\n",
    "                                      password=\"Sam@priya2913\",\n",
    "                                      host=\"35.225.61.46\",\n",
    "                                      port=\"5432\",\n",
    "                                      database=\"internships\")\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        postgres_insert_query = \"\"\" INSERT INTO jobs_internshala_base_urls (url,source,date) VALUES (%s,%s,%s)\"\"\"\n",
    "        record_to_insert = (url, source, date)\n",
    "        cursor.execute(postgres_insert_query, record_to_insert)\n",
    "\n",
    "        connection.commit()\n",
    "        count = cursor.rowcount\n",
    "#         print (count, \"Record inserted successfully into jobs_internshala_base_urls table\")\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error :\n",
    "        if(connection):\n",
    "            print(\"Failed to insert record into jobs_internshala_base_urls table\", error)\n",
    "\n",
    "    finally:\n",
    "        #closing database connection.\n",
    "        if(connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"PostgreSQL connection is closed\")\n",
    "\n",
    "\n",
    "    \n",
    "url_data = pd.DataFrame(urls,columns=[\"URLS\",\"Date\",\"Source\"])    \n",
    "page_url = pd.DataFrame(urls,columns = ['page_url','url_source','date'])\n",
    "page_url.to_csv('data/internshala_pages.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_14756/611002220.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Sailu\\AppData\\Local\\Temp/ipykernel_14756/611002220.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    sudo pip install psycopg2==2.7.5 --ignore-installed\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(page_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Unique job urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import urllib\n",
    "from urllib.parse import unquote\n",
    "import re\n",
    "import time\n",
    "import urllib.request\n",
    "import requests\n",
    "# import urllib.urlparse\n",
    "# import urllib.Request\n",
    "# import urllib.HTTPError\n",
    "from urllib.error import URLError, HTTPError\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.error import URLError, HTTPError\n",
    "from http.client import BadStatusLine\n",
    "import urllib.request,re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import csv\n",
    "import ssl\n",
    "import json\n",
    "urls = pd.read_csv('data/internshala_pages.csv')\n",
    "urls = urls.page_url.unique()\n",
    "unique_urls = []\n",
    "for i in urls:\n",
    "    try:\n",
    "        try:\n",
    "            html_page = urllib.request.urlopen(i)\n",
    "            soup = BeautifulSoup(html_page)\n",
    "            for link in soup.findAll('a', attrs={'href': re.compile(\"internship/detail\")}):\n",
    "        #         print(link.get('href'))\n",
    "                date = time.strftime(\"%Y-%m-%d\")\n",
    "                source = \"Internshala\"\n",
    "                url = 'https://internshala.com'+link.get('href')\n",
    "                unique_urls.append(('https://internshala.com'+link.get('href'),date,source))\n",
    "                job_urls = pd.DataFrame(unique_urls,columns=[\"URLS\",\"Date\",\"Source\"])\n",
    "                job_urls = job_urls.drop_duplicates()\n",
    "                job_urls=job_urls.reset_index(drop=True)\n",
    "                job_urls.to_csv('data/job_unique_urls.csv',index=False)\n",
    "                try:\n",
    "                    for index, row in job_urls.iterrows():\n",
    "                        source            =  row['Source']\n",
    "                        date             =  row['Date']\n",
    "                        url                 =  row['URLS']\n",
    "\n",
    "                        connection = psycopg2.connect(user=\"_20p31f0014\",\n",
    "                                                      password=\"_20p31f0014\",\n",
    "                                                      host=\"postgres.pro9.team\",\n",
    "                                                      port=\"5432\",\n",
    "                                                      database=\"internships\")\n",
    "                        cursor = connection.cursor()\n",
    "\n",
    "                        postgres_insert_query = \"\"\" INSERT INTO jobs_internshala_jobs_urls(url,source,date) VALUES (%s,%s,%s) ON CONFLICT(url) DO UPDATE SET source = jobs_internshala_jobs_urls.source,date=jobs_internshala_jobs_urls.date\"\"\"\n",
    "                        record_to_insert = (url, source, date)\n",
    "                        cursor.execute(postgres_insert_query, record_to_insert)\n",
    "\n",
    "                        connection.commit()\n",
    "                        count = cursor.rowcount\n",
    "                #         print (count, \"Record inserted successfully into jobs_internshala_base_urls table\")\n",
    "\n",
    "                except (Exception, psycopg2.Error) as error :\n",
    "                    if(connection):\n",
    "                        print(\"Failed to insert record into jobs_internshala_base_urls table\", error)\n",
    "\n",
    "                finally:\n",
    "                    #closing database connection.\n",
    "                    if(connection):\n",
    "                        cursor.close()\n",
    "                        connection.close()\n",
    "                        print(\"PostgreSQL connection is closed\")\n",
    "        except BadStatusLine: # Catch if invalid url names exist\n",
    "            print(\"could not fetch %s\" % url_name)\n",
    "\n",
    "    except HTTPError as err: # catch HTTP 404 not found error\n",
    "        if err == 404:\n",
    "            print(\"Received HTTPError on %s\" % url_name)\n",
    "            print(err)\n",
    "\n",
    "    except HTTPError as err: # catch HTTP 404 not found error\n",
    "        if err == 502:\n",
    "            print(\"Received HTTPError on %s\" % url_name)\n",
    "            print(err)\n",
    "\n",
    "    except HTTPError as err: # catch HTTP 404 not found error\n",
    "        if err == 110:\n",
    "            print(\"Received HTTPError on %s\" % url_name)\n",
    "            print(err)\n",
    "    except Exception as error:\n",
    "        print('Error occured: {}'.format(error))\n",
    "\n",
    "job_urls = pd.DataFrame(unique_urls,columns=[\"URLS\",\"Date\",\"Source\"])\n",
    "job_urls = job_urls.drop_duplicates()\n",
    "job_urls=job_urls.reset_index(drop=True)\n",
    "job_urls.to_csv('data/job_unique_urls.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        except BadStatusLine: # Catch if invalid url names exist\n",
    "            print(\"could not fetch %s\" % url_name)\n",
    "\n",
    "    except HTTPError as err: # catch HTTP 404 not found error\n",
    "        if err == 404:\n",
    "            print(\"Received HTTPError on %s\" % url_name)\n",
    "            print(err)\n",
    "    \n",
    "    except HTTPError as err: # catch HTTP 404 not found error\n",
    "        if err == 110:\n",
    "            print(\"Received HTTPError on %s\" % url_name)\n",
    "            print(err)\n",
    "    except Exception as error:\n",
    "        print('Error occured: {}'.format(error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data FRom job urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len('b1557cace5434026b2b3ce4022b22a7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data.URLS.unique()[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# starting time\n",
    "start = time.time()\n",
    "records = []\n",
    "skills = []\n",
    "url_data = pd.read_csv('data/job_unique_urls.csv')\n",
    "url_data = url_data.iloc[:,:]\n",
    "url = url_data.URLS.unique()\n",
    "# url = ['https://internshala.com/internship/detail/graphic-design-work-from-home-job-internship-at-cosmopolitan-posters1590667810']\n",
    "for i in url: \n",
    "    r = requests.get(i)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    results = soup.find_all('div', attrs={'class':'max-width-container'})\n",
    "    if len(results)>1:\n",
    "        first_result = results[0]\n",
    "    #     print(len(first_result.find_all('div',attrs={'id':'skillsContainer'})))\n",
    "        JobTitle = first_result.find_all('span')[0].text[:]\n",
    "    #     Company =  first_result.find_all('a')[0]['title'] \n",
    "        Company = first_result.find_all('a', attrs={'class':'link_display_like_text'})[0]['title']\n",
    "        CompanyURL = first_result.find_all('a')[2]['href'] \n",
    "        StartDate = first_result.find_all('td')[0].text[1:-1]\n",
    "        Duration = first_result.find_all('td')[1].text[1:-1]\n",
    "        Stipend = first_result.find_all('td')[2].text[1:-1]\n",
    "        PostedOn = first_result.find_all('td')[3].text[:]\n",
    "        ApplyBY = first_result.find_all('td')[4].text[:]\n",
    "        About_company = first_result.find_all('div',attrs={'class':'freetext-container'})[0].text[:]\n",
    "        About_work = first_result.find_all('div',attrs={'class':'freetext-container'})[1].text[:]\n",
    "        Who_can_apply = first_result.find_all('div',attrs={'class':'freetext-container'})[2].text[1:-1]\n",
    "        Openings = first_result.find_all('div',attrs={'id':'numberOfPositionsContainer'})[0].text[50:-2]\n",
    "    #     print(len(first_result.find_all('div',attrs={'class':'freetext-container'})))\n",
    "        if len(first_result.find_all('div',attrs={'class':'freetext-container'}))>=4:\n",
    "            Other_requirements = first_result.find_all('div',attrs={'class':'freetext-container'})[3].text[1:-1]\n",
    "        else:\n",
    "            Other_requirements = \"Not Found\"\n",
    "        if len(first_result.find_all('div',attrs={'id':'skillsContainer'}))>=1:\n",
    "            Skills = first_result.find_all('div',attrs={'id':'skillsContainer'})[0].text[20:-2]\n",
    "    #         print(len(first_result.find_all('div',attrs={'id':'skillsContainer'})))\n",
    "        else:\n",
    "            Skills = \"None\"\n",
    "\n",
    "\n",
    "\n",
    "        records.append((JobTitle,Company,CompanyURL,StartDate,Duration,Stipend,PostedOn,ApplyBY,About_company,About_work,Who_can_apply,Other_requirements,Skills,Openings))\n",
    "    df = pd.DataFrame(records, columns=[\"JobTItle\",\"Company\",\"Url\",\"StartDate\",\"Duration\",\"Stipend\",\"PostedOn\",\"ApplyBy\",'About_Company','About_Work','Who_Can_Apply',\"Other_requirements\",'Skills','Openings'])\n",
    "    df.Stipend = df.Stipend.str.replace(r'\\n','')\n",
    "    df.About_Company = df.About_Company.str.replace(r'\\n','')\n",
    "    df.About_Work = df.About_Work.str.replace(r'\\n','')\n",
    "    df.columns = df.columns.map(lambda x: x.strip())\n",
    "    df.to_csv('data/job_details.csv')\n",
    "    end = time.time()\n",
    "    # total time taken\n",
    "print(f\"Runtime of the program is {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "\n",
    "html_page = urllib.request.urlopen(\"https://www.letsintern.com/internships/IT-internships\")\n",
    "soup = BeautifulSoup(html_page)\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"/internship/\")}):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
